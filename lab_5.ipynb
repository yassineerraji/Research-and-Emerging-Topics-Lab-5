{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37f737f",
   "metadata": {},
   "source": [
    "# Lab 5: Leveraging Open Data from Wikipedia for LLM Prompt Engineering\n",
    "\n",
    "## Overview\n",
    "This lab demonstrates how to extract structured data from Wikipedia pages and use it to create effective prompts for Large Language Models (LLMs). You'll learn to work with real-world financial data, process it programmatically, and engineer prompts for various AI tasks.\n",
    "\n",
    "## Learning Objectives\n",
    "- ‚úì Extract financial index components from Wikipedia\n",
    "- ‚úì Retrieve company infobox data programmatically\n",
    "- ‚úì Build structured datasets from semi-structured web data\n",
    "- ‚úì Design effective LLM prompts for different tasks\n",
    "- ‚úì Process and clean text data for AI consumption\n",
    "- ‚úì Create reusable prompt templates and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e1c61",
   "metadata": {},
   "source": [
    "## Part 1: Data Extraction from Wikipedia\n",
    "\n",
    "### What is a Financial Index?\n",
    "A financial index is a composite measure of a subset of companies in a specific market or sector. Examples include:\n",
    "- **S&P 500**: 500 largest US companies\n",
    "- **EURO STOXX 50**: 50 largest Eurozone companies\n",
    "- **DAX**: 40 largest German companies\n",
    "\n",
    "### Your Task\n",
    "1. **Identify components**: Extract the list of companies in each index from Wikipedia\n",
    "2. **Gather company data**: Retrieve detailed information (infoboxes) from each company's Wikipedia page\n",
    "3. **Build a dataset**: Combine all data into structured format suitable for LLM processing\n",
    "4. **Engineer prompts**: Create effective prompts that leverage this data for AI tasks\n",
    "\n",
    "### Data Sources\n",
    "- **Index components**: Wikipedia articles listing index members\n",
    "- **Company data**: Wikipedia infoboxes (structured data boxes on company pages)\n",
    "- **Dump file**: Optional - for advanced analysis of full Wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e42bbe",
   "metadata": {},
   "source": [
    "### Optional: Full Wikipedia Dump\n",
    "For advanced analysis, you can download the complete Wikipedia dump from:\n",
    "- **Link**: https://dumps.wikimedia.org/enwiki/\n",
    "- **File**: `enwiki-latest-pages-articles-multistream-index.txt.bz2`\n",
    "- **Use case**: Full-text search, article history analysis, or complete data scraping\n",
    "- **Note**: Very large files (100+ GB) - requires significant storage and processing power\n",
    "\n",
    "For this lab, we'll focus on extracting specific data via the Wikipedia API, which is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02aa961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS & SETUP\n",
    "# ============================================================================\n",
    "# These libraries enable us to work with Wikipedia data\n",
    "\n",
    "import pandas as pd              # Data manipulation and analysis\n",
    "import urllib.request           # HTTP requests to Wikipedia\n",
    "from pathlib import Path        # Cross-platform file path handling\n",
    "from typing import Union        # Type hints for better code clarity\n",
    "from tqdm import tqdm          # Progress bars for long operations\n",
    "import wptools               # Wikipedia parsing (infobox extraction)\n",
    "from loguru import logger       # Enhanced logging\n",
    "import json                     # Working with JSON data\n",
    "import numpy as np             # Numerical operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25c9b8",
   "metadata": {},
   "source": [
    "## Step 1: Extract Index Components from Wikipedia\n",
    "\n",
    "### Task: Extract Company Lists\n",
    "We'll extract the list of companies that make up each financial index directly from Wikipedia.\n",
    "\n",
    "### Indices We're Covering:\n",
    "1. **S&P 500** (USA) - 500 largest US companies\n",
    "2. **EURO STOXX 50** (Europe) - 50 largest Eurozone companies  \n",
    "3. **CAC 40** (France) - 40 largest French companies\n",
    "4. **DAX** (Germany) - 40 largest German companies\n",
    "5. **CSI 300** (China) - 300 largest Chinese companies\n",
    "6. **S&P Latin America 40** (Latin America) - 40 major LA companies\n",
    "7. **BSE SENSEX** (India) - 30 largest Indian companies\n",
    "8. **NASDAQ-100** (USA Tech) - 100 largest non-financial NASDAQ companies\n",
    "\n",
    "### How It Works:\n",
    "- Each index has a Wikipedia article with a table listing its components\n",
    "- We'll use `pd.read_html()` to extract all tables from these pages\n",
    "- Tables are saved as CSV files for later processing\n",
    "- This approach is fast, requires no authentication, and respects Wikipedia's terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1022b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. GET TABLES\n",
    "# ============================================================================\n",
    "\n",
    "def download_tables(url, save_dir):\n",
    "    \"\"\"Downloads all tables from a Wiki page to CSVs.\"\"\"\n",
    "    path = Path(save_dir)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Reading {url}...\")\n",
    "    # Pandas does the heavy lifting\n",
    "    tables = pd.read_html(url)\n",
    "    \n",
    "    for i, df in enumerate(tables):\n",
    "        df.to_csv(path / f\"table_{i}.csv\", index=False)\n",
    "    print(f\"-> Saved {len(tables)} tables in {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b28fd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP: Configure Wikipedia Index URLs and HTTP Headers\n",
    "# ============================================================================\n",
    "\n",
    "# Dictionary mapping index names to their Wikipedia article URLs\n",
    "# These URLs contain tables with the company components of each index\n",
    "indices = {\n",
    "    \"sp500\": \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\",\n",
    "    \"eurostoxx50\": \"https://en.wikipedia.org/wiki/EURO_STOXX_50\",\n",
    "    \"cac40\": \"https://en.wikipedia.org/wiki/CAC_40\",\n",
    "    \"dax\": \"https://en.wikipedia.org/wiki/DAX\",\n",
    "    \"csi300\": \"https://en.wikipedia.org/wiki/CSI_300_Index\",\n",
    "    \"spla40\": \"https://en.wikipedia.org/wiki/S%26P_Latin_America_40\",\n",
    "    \"bsesensex\": \"https://en.wikipedia.org/wiki/BSE_SENSEX\",\n",
    "    \"nasdaq100\": \"https://en.wikipedia.org/wiki/Nasdaq-100\",\n",
    "}\n",
    "\n",
    "# IMPORTANT: Configure HTTP headers to identify our bot to Wikipedia\n",
    "# This is REQUIRED for ethical web scraping - identify yourself!\n",
    "# Wikipedia may block requests without proper User-Agent headers\n",
    "\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [\n",
    "    (\"User-Agent\", \"MyResearchBot/1.0 (contact@example.com)\")  # Identify your bot\n",
    "]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8e6a2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading https://en.wikipedia.org/wiki/List_of_S%26P_500_companies...\n",
      "-> Saved 3 tables in data/indices/sp500\n",
      "Reading https://en.wikipedia.org/wiki/EURO_STOXX_50...\n",
      "-> Saved 10 tables in data/indices/eurostoxx50\n",
      "Reading https://en.wikipedia.org/wiki/CAC_40...\n",
      "-> Saved 20 tables in data/indices/cac40\n",
      "Reading https://en.wikipedia.org/wiki/DAX...\n",
      "-> Saved 10 tables in data/indices/dax\n",
      "Reading https://en.wikipedia.org/wiki/CSI_300_Index...\n",
      "-> Saved 7 tables in data/indices/csi300\n",
      "Reading https://en.wikipedia.org/wiki/S%26P_Latin_America_40...\n",
      "-> Saved 4 tables in data/indices/spla40\n",
      "Reading https://en.wikipedia.org/wiki/BSE_SENSEX...\n",
      "-> Saved 33 tables in data/indices/bsesensex\n",
      "Reading https://en.wikipedia.org/wiki/Nasdaq-100...\n",
      "-> Saved 19 tables in data/indices/nasdaq100\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXECUTE STEP 1\n",
    "# ============================================================================\n",
    "\n",
    "for name, url in indices.items():\n",
    "    download_tables(url, f\"data/indices/{name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06eb0af",
   "metadata": {},
   "source": [
    "## Step 2: Extract Company Infoboxes from Wikipedia\n",
    "\n",
    "### What are Infoboxes?\n",
    "Wikipedia infoboxes are structured data boxes that appear on the right side of articles. They contain:\n",
    "- Company name and alternative names\n",
    "- Industry classification\n",
    "- Founded date and location\n",
    "- Key executives\n",
    "- Headquarters location\n",
    "- Number of employees\n",
    "- Revenue and financial metrics\n",
    "- Official website URLs\n",
    "- Stock exchange listings\n",
    "- And much more...\n",
    "\n",
    "### Why Infoboxes?\n",
    "- **Structured data**: Unlike article body text, infoboxes are semi-structured\n",
    "- **Consistency**: Fields follow a template across similar articles\n",
    "- **Ease of extraction**: Wikipedia APIs can parse infoboxes directly\n",
    "- **Rich context**: Perfect for LLM prompts - contains exactly the info LLMs need\n",
    "\n",
    "### Process\n",
    "1. Use the `wptools` library to fetch each company's Wikipedia page\n",
    "2. Extract the infobox (structured data) from the page parse\n",
    "3. Save as JSON for flexibility and later processing\n",
    "4. Handle errors gracefully (some companies may not have Wikipedia pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19bc802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. GET INFOBOXES\n",
    "# ============================================================================\n",
    "\n",
    "def download_infobox(company, save_dir):\n",
    "    \"\"\"Gets the infobox for a company.\"\"\"\n",
    "    try:\n",
    "        # Handle names like 'Alphabet (Class A)' -> 'Alphabet'\n",
    "        search_name = company.split('(')[0].strip()\n",
    "        \n",
    "        # Fetch page\n",
    "        page = wptools.page(search_name, silent=True).get_parse()\n",
    "        infobox = page.data.get('infobox')\n",
    "        \n",
    "        if infobox:\n",
    "            # Save to JSON\n",
    "            clean_name = \"\".join(x for x in company if x.isalnum()).strip()\n",
    "            path = Path(save_dir) / f\"{clean_name}.json\"\n",
    "            path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            with open(path, 'w') as f:\n",
    "                json.dump(infobox, f, indent=4)\n",
    "            print(f\"‚úì {company}\")\n",
    "        else:\n",
    "            print(f\"‚úó {company} (no infobox)\")\n",
    "            \n",
    "    except Exception:\n",
    "        print(f\"‚úó {company} (not found)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5adcddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found! Run the cell above first.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DISPLAY: View the Extracted Infobox\n",
    "# ============================================================================\n",
    "# This shows what data we extracted from Wikipedia\n",
    "\n",
    "example_file = Path(\"data/infoboxes/sp500/3M.json\")\n",
    "\n",
    "if example_file.exists():\n",
    "    with open(example_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        # Display the first few keys\n",
    "        print(\"Infobox keys found:\", list(data.keys())[:10])\n",
    "        print(\"\\nFull Content (truncated):\\n\")\n",
    "        print(json.dumps(data, indent=4)[:500] + \"\\n...[truncated]\")\n",
    "else:\n",
    "    print(\"File not found! Run the cell above first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8224888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing sp500 ===\n",
      "Extracting 10 companies...\n",
      "Downloading infobox for: 3M...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: A. O. Smith...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Abbott Laboratories...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: AbbVie...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Accenture...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Adobe Inc....\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Advanced Micro Devices...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: AES Corporation...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Aflac...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Agilent Technologies...\n",
      "-> Success! Saved.\n",
      "\n",
      "=== Processing eurostoxx50 ===\n",
      "Extracting 10 companies...\n",
      "Downloading infobox for: Adidas...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Adyen...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Ahold Delhaize...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Air Liquide...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Airbus...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Allianz...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Anheuser-Busch InBev...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Argenx...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: ASML Holding...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Axa...\n",
      "-> Success! Saved.\n",
      "\n",
      "=== Processing cac40 ===\n",
      "Extracting 10 companies...\n",
      "Downloading infobox for: Accor...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Air Liquide...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Airbus...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: ArcelorMittal...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Axa...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: BNP Paribas...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Bouygues...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Bureau Veritas...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Capgemini...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Carrefour...\n",
      "-> Success! Saved.\n",
      "\n",
      "=== Processing dax ===\n",
      "Extracting 10 companies...\n",
      "Downloading infobox for: Adidas...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Airbus...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Allianz...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: BASF...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Bayer...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Beiersdorf...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: BMW...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Brenntag...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Commerzbank...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Continental...\n",
      "-> No infobox found for Continental\n",
      "\n",
      "=== Processing csi300 ===\n",
      "Extracting 10 companies...\n",
      "Downloading infobox for: Kweichow Moutai...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Ping An Insurance...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: CATL...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: China Merchants Bank...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Midea Group...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Wuliangye Yibin...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: China Yangtze Power...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Industrial Bank...\n",
      "-> No infobox found for Industrial Bank\n",
      "Downloading infobox for: Zijin Mining Group...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: CITIC Securities...\n",
      "-> Success! Saved.\n",
      "\n",
      "=== Processing nasdaq100 ===\n",
      "Extracting 10 companies...\n",
      "Downloading infobox for: Adobe Inc....\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Advanced Micro Devices...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Airbnb...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Alphabet Inc. (Class A)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API error: {'code': 'missingtitle', 'info': \"The page you specified doesn't exist.\", 'docref': 'See https://en.wikipedia.org/w/api.php for API usage. Subscribe to the mediawiki-api-announce mailing list at &lt;https://lists.wikimedia.org/postorius/lists/mediawiki-api-announce.lists.wikimedia.org/&gt; for notice of API deprecations and breaking changes.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Retrying with simplified name: 'Alphabet Inc.'...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Alphabet Inc. (Class C)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API error: {'code': 'missingtitle', 'info': \"The page you specified doesn't exist.\", 'docref': 'See https://en.wikipedia.org/w/api.php for API usage. Subscribe to the mediawiki-api-announce mailing list at &lt;https://lists.wikimedia.org/postorius/lists/mediawiki-api-announce.lists.wikimedia.org/&gt; for notice of API deprecations and breaking changes.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Retrying with simplified name: 'Alphabet Inc.'...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Amazon...\n",
      "-> No infobox found for Amazon\n",
      "Downloading infobox for: American Electric Power...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Amgen...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Analog Devices...\n",
      "-> Success! Saved.\n",
      "Downloading infobox for: Apple Inc....\n",
      "-> Success! Saved.\n",
      "\n",
      "Batch extraction complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXECUTION: Batch Extract Infoboxes\n",
    "# ============================================================================\n",
    "# We'll extract a few companies from each index to test our pipeline.\n",
    "# To extract ALL companies, set LIMIT = None (takes much longer!)\n",
    "\n",
    "LIMIT = 10  # We'll only take the first 10 companies per index for this lab\n",
    "\n",
    "# Configuration: which file and column to use for each index\n",
    "index_config = {\n",
    "    \"sp500\":       (\"table_1.csv\", \"Security\"),\n",
    "    \"eurostoxx50\": (\"table_4.csv\", \"Name\"),\n",
    "    \"cac40\":       (\"table_4.csv\", \"Company\"),\n",
    "    \"dax\":         (\"table_4.csv\", \"Company\"),\n",
    "    \"csi300\":      (\"table_3.csv\", \"Company\"),\n",
    "    \"nasdaq100\":   (\"table_4.csv\", \"Company\")\n",
    "}\n",
    "\n",
    "for index_name, (csv_file, col_name) in index_config.items():\n",
    "    print(f\"\\n=== Processing {index_name} ===\")\n",
    "    \n",
    "    # 1. Load the company list\n",
    "    csv_path = Path(f\"data/indices/{index_name}/{csv_file}\")\n",
    "    if not csv_path.exists():\n",
    "        print(f\"CSV not found: {csv_path}\")\n",
    "        continue\n",
    "        \n",
    "    df = pd.read_csv(csv_path)\n",
    "    companies = df[col_name].tolist()\n",
    "    \n",
    "    # 2. Extract infoboxes (limited number)\n",
    "    target_dir = f\"data/infoboxes/{index_name}\"\n",
    "    \n",
    "    # Apply limit if set\n",
    "    companies_to_process = companies[:LIMIT] if LIMIT else companies\n",
    "    \n",
    "    print(f\"Extracting {len(companies_to_process)} companies...\")\n",
    "    \n",
    "    for company in companies_to_process:\n",
    "        get_company_infobox(company, target_dir)\n",
    "\n",
    "print(\"\\nBatch extraction complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f8f7d",
   "metadata": {},
   "source": [
    "## Step 3: Aggregate Infoboxes into Databases\n",
    "\n",
    "### What We're Building\n",
    "We're converting individual JSON files (one per company) into consolidated CSV databases (one per index).\n",
    "\n",
    "### Why?\n",
    "- **Easier analysis**: CSV format works with pandas, Excel, and most analysis tools\n",
    "- **Efficiency**: One file per index instead of hundreds of individual JSON files\n",
    "- **Standardization**: Creates a uniform dataset structure for LLM processing\n",
    "\n",
    "### Process\n",
    "1. Read all JSON infobox files for an index from disk\n",
    "2. Convert each JSON to a DataFrame row\n",
    "3. Concatenate all rows into a single DataFrame\n",
    "4. Save as CSV with proper encoding\n",
    "\n",
    "### Notes for Future Enhancement\n",
    "- The infoboxes contain many fields beyond what we use now (URLs, images, etc.)\n",
    "- Future work could extract and leverage additional information\n",
    "- This foundation allows flexible data extraction later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3394df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data/processed/sp500_data.csv (10 rows)\n",
      "Created data/processed/cac40_data.csv (10 rows)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. MERGE TO CSV\n",
    "# ============================================================================\n",
    "\n",
    "def merge_to_csv(index_name):\n",
    "    \"\"\"Merges all JSONs in a folder to one CSV.\"\"\"\n",
    "    data = []\n",
    "    folder = Path(f\"data/infoboxes/{index_name}\")\n",
    "    \n",
    "    # Read all JSONs\n",
    "    for file in folder.glob(\"*.json\"):\n",
    "        with open(file) as f:\n",
    "            row = json.load(f)\n",
    "            row['file_id'] = file.stem # Keep track of source\n",
    "            data.append(row)\n",
    "            \n",
    "    if data:\n",
    "        # Save\n",
    "        df = pd.DataFrame(data)\n",
    "        out = Path(f\"data/processed/{index_name}_data.csv\")\n",
    "        out.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(out, index=False)\n",
    "        print(f\"Created {out} ({len(df)} rows)\")\n",
    "\n",
    "# Run\n",
    "for index in [\"sp500\", \"cac40\"]:\n",
    "    merge_to_csv(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21235e6",
   "metadata": {},
   "source": [
    "# Part 2: Data Processing & LLM Prompt Engineering\n",
    "\n",
    "## Overview of Part 2\n",
    "Now that we have structured company data from Wikipedia, we'll:\n",
    "\n",
    "1. **Load and analyze** the infobox database\n",
    "2. **Clean and preprocess** the data for LLM consumption\n",
    "3. **Create prompt templates** for different LLM tasks\n",
    "4. **Design context formatting** that maximizes LLM effectiveness\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Why Clean Data for LLMs?\n",
    "- LLMs perform better with well-structured, clean text\n",
    "- Removing noise and formatting artifacts improves accuracy\n",
    "- Consistent formatting allows for better prompt engineering\n",
    "- Clean data enables batch processing and cost optimization\n",
    "\n",
    "### Prompt Engineering\n",
    "Prompt engineering is the art of crafting inputs to LLMs to get better outputs. We'll explore:\n",
    "- **Context formatting**: How to present company data effectively\n",
    "- **Task-specific templates**: Different prompts for different goals\n",
    "- **Few-shot learning**: Providing examples to guide LLM behavior\n",
    "- **Output structuring**: Getting structured responses (JSON, tables, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7152b26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['name', 'logo', 'logo_size', 'image', 'image_size']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>logo</th>\n",
       "      <th>logo_size</th>\n",
       "      <th>image</th>\n",
       "      <th>image_size</th>\n",
       "      <th>image_caption</th>\n",
       "      <th>former_name</th>\n",
       "      <th>type</th>\n",
       "      <th>traded_as</th>\n",
       "      <th>ISIN</th>\n",
       "      <th>...</th>\n",
       "      <th>net_income_year</th>\n",
       "      <th>assets_year</th>\n",
       "      <th>equity_year</th>\n",
       "      <th>hq_location</th>\n",
       "      <th>footnotes</th>\n",
       "      <th>logo_caption</th>\n",
       "      <th>subsid</th>\n",
       "      <th>predecessor</th>\n",
       "      <th>num_locations</th>\n",
       "      <th>locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3M Company</td>\n",
       "      <td>3M wordmark.svg</td>\n",
       "      <td>175px</td>\n",
       "      <td>3-M Building Maplewood MN1.jpg</td>\n",
       "      <td>250px</td>\n",
       "      <td>3M headquarters in [[Maplewood, Minnesota]]</td>\n",
       "      <td>Minnesota Mining and Manufacturing Company (19...</td>\n",
       "      <td>[[Public company|Public]]</td>\n",
       "      <td>{{Unbulleted list|New York Stock Exchange|MMM|...</td>\n",
       "      <td>{{ISIN|sl|=|n|pl|=|y|US88579Y1010}}</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         name             logo logo_size                           image  \\\n",
       "0  3M Company  3M wordmark.svg     175px  3-M Building Maplewood MN1.jpg   \n",
       "\n",
       "  image_size                                image_caption  \\\n",
       "0      250px  3M headquarters in [[Maplewood, Minnesota]]   \n",
       "\n",
       "                                         former_name  \\\n",
       "0  Minnesota Mining and Manufacturing Company (19...   \n",
       "\n",
       "                        type  \\\n",
       "0  [[Public company|Public]]   \n",
       "\n",
       "                                           traded_as  \\\n",
       "0  {{Unbulleted list|New York Stock Exchange|MMM|...   \n",
       "\n",
       "                                  ISIN  ... net_income_year assets_year  \\\n",
       "0  {{ISIN|sl|=|n|pl|=|y|US88579Y1010}}  ...             NaN         NaN   \n",
       "\n",
       "  equity_year hq_location footnotes logo_caption subsid predecessor  \\\n",
       "0         NaN         NaN       NaN          NaN    NaN         NaN   \n",
       "\n",
       "  num_locations locations  \n",
       "0           NaN       NaN  \n",
       "\n",
       "[1 rows x 55 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"data/processed/sp500_data.csv\")\n",
    "print(\"Columns:\", list(df.columns[:5]))\n",
    "display(df.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd6d22b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPANY PROFILE:\n",
      "Name: 3M Company\n",
      "Industry: Conglomerate (company)|Conglomerate\n",
      "Location: Unknown\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. PREPARE FOR AI\n",
    "# ============================================================================\n",
    "\n",
    "def clean(text):\n",
    "    \"\"\"Remove {{brackets}} and [[links]].\"\"\"\n",
    "    if not isinstance(text, str): return \"Unknown\"\n",
    "    for char in \"[]{}\":\n",
    "        text = text.replace(char, \"\")\n",
    "    return text.strip()\n",
    "\n",
    "def make_context(row):\n",
    "    \"\"\"Create a readable profile from a row.\"\"\"\n",
    "    # Smart lookup: try different common column names\n",
    "    name = row.get('name') or row.get('company_name') or row.get('file_id')\n",
    "    industry = row.get('industry') or row.get('sector')\n",
    "    hq = row.get('location') or row.get('headquarters')\n",
    "    \n",
    "    return f\"\"\"COMPANY PROFILE:\n",
    "Name: {clean(name)}\n",
    "Industry: {clean(industry)}\n",
    "Location: {clean(hq)}\n",
    "\"\"\"\n",
    "\n",
    "# Test\n",
    "print(make_context(df.iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5f1683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PROMPT FOR AI ---\n",
      "Summarize this company in 1 sentence.\n",
      "\n",
      "DATA:\n",
      "COMPANY PROFILE:\n",
      "Name: 3M Company\n",
      "Industry: Conglomerate (company)|Conglomerate\n",
      "Location: Unknown\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. GENERATE PROMPTS\n",
    "# ============================================================================\n",
    "\n",
    "SUMMARY_TASK = \"Summarize this company in 1 sentence.\"\n",
    "RISK_TASK = \"Classify the risk level (Low/Med/High) based on this profile.\"\n",
    "\n",
    "def create_prompt(row, task):\n",
    "    \"\"\"Combines Context + Task.\"\"\"\n",
    "    context = make_context(row)\n",
    "    return f\"{task}\\n\\nDATA:\\n{context}\"\n",
    "\n",
    "# Demo\n",
    "row = df.iloc[0]\n",
    "print(\"--- PROMPT FOR AI ---\")\n",
    "print(create_prompt(row, SUMMARY_TASK))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ec58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ca6b84c",
   "metadata": {},
   "source": [
    "End of lab 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d9f9a",
   "metadata": {},
   "source": [
    "üöÄ NEXT STEPS (in anticipation of the final lab)\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "1. Export your full dataset using PromptExporter\n",
    "2. Test prompts with a small sample (5-10 companies)\n",
    "3. Evaluate LLM outputs for quality and accuracy\n",
    "4. Iterate on prompts based on results\n",
    "5. Scale up to full dataset using batch APIs\n",
    "6. Monitor token usage and costs\n",
    "7. Implement feedback loops for continuous improvement\n",
    "8. Build evaluation metrics for output quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
